\chapter{Volatile Database}
\label{volatile}

The Volatile DB is tasked with storing the blocks that are part of the
\emph{volatile} part of the chain. Do not be misled by its name, the Volatile DB
\emph{should persist} blocks stored to disk. The volatile part of the chain
consists of the last $k$ (the security parameter, see
\cref{consensus:overview:k}) blocks of the chain, which can still be rolled back
when switching to a fork. This means that unlike the Immutable DB, which stores
the immutable prefix of \emph{the} chosen chain, the Volatile DB can store
potentially multiple chains, one of which will be the current chain. It will
also store forks that we have switched away from, or will still switch to, when
they grow longer and become preferable to our current chain. Moreover, the
Volatile DB can contain disconnected blocks, as the block fetch
client\todo{link} might download or receive blocks out of order.

We list the requirements and non-requirements of this component in no particular
order. Note that some of these requirements were defined in response to the
requirements of the Immutable DB (see \cref{immutable}), and vice versa.

\begin{itemize}
\item \textbf{Add-only}: new blocks are always added, never modified.
\item \textbf{Out-of-order}: new blocks can be added in any order, i.e.,
  consecutive blocks on a chain are not necessarily added consecutively. They
  can arrive in any order and can be interspersed with blocks from other chains.
\item \textbf{Garbage-collected}: blocks in the current chain that become older
  than $k$, i.e., there are at least $k$ more recent blocks in the current chain
  after them, are copied from the Volatile DB to the Immutable DB, as they move
  from the volatile to the immutable part of the chain. After copying them to
  the Immutable DB, they can be \emph{garbage collected} from the Volatile DB.

  Blocks that are not part of the chain but are too old to switch to, should
  also be garbage collected.
\item \textbf{Overlap}: by allowing an \emph{overlap} of blocks between the
  Immutable DB and the Volatile DB, i.e., by delaying garbage collection so that
  it does not happen right after copying the block to the Immutable DB, we can
  weaken the durability requirement on the Immutable DB. Blocks truncated from
  the end of the Immutable DB will likely still be in the Volatile DB, and can
  simply be copied again.\todo{done by ChainDB}
\item \textbf{Durability}: similar to the Immutable DB's durability
  \emph{non-requirement}, losing a block because of a crash in the middle or
  right after appending a block is inconsequential. The block can be downloaded
  again.
\item \textbf{Size}: because of garbage collection, there is a bound on the size
  of the Volatile DB in terms of blocks: in the order of $k$, which is 2160 for
  mainnet (we give a more detailed estimate of the size in
  \cref{volatile:implementation:gc}). This makes the size of the Volatile DB
  relatively small, allowing for some information to be kept in memory instead
  of on disk.
\item \textbf{Reading}: the database should be able to return the block or
  header corresponding to the given hash efficiently. Unlike the Immutable DB,
  we do not index by slot numbers, as multiple blocks, from different forks, can
  have the same slot number. Instead, we use header hashes.
\item \textbf{Queries}: it should be possible to query information about blocks.
  For example, we need to be able to efficiently tell which blocks are stored in
  the Volatile DB, or construct a path through the Volatile DB connecting a
  block to another one by chasing its predecessors.\footnote{Note that
  implementing this efficiently using SQL is not straightforward.} Such
  operations should produce consistent results, even while blocks are being
  added and garbage collected concurrently.
\item \textbf{Recoverability}: because of its small size and it being acceptable
  to download missing blocks again, it is not of paramount importance to be able
  to recover as many blocks as possible in case of a corruption.

  However, corrupted blocks should be detected and deleted from the Volatile DB.
\item \textbf{Efficient streaming}: while blocks will be streamed from the
  Volatile DB, this requirement is not as important as it is for the Immutable
  DB. Only a small number of blocks will reside in the Volatile DB, hence fewer
  blocks will be streamed. Most commonly, the block at the tip of the chain will
  be streamed from the Volatile DB (and possibly some of its predecessors). In
  this case, efficiently being able to read a single block will suffice.
\end{itemize}

\section{API}
\label{volatile:api}

Before we describe the implementation of the Volatile DB, we first describe its
functionality. The Volatile DB has the following API:

\begin{lstlisting}
data VolatileDB m blk = VolatileDB {
      closeDB :: m ()

    , putBlock :: blk -> m ()

    , getBlockComponent ::
           forall b.
           BlockComponent blk b
        -> HeaderHash blk
        -> m (Maybe b)

    , garbageCollect :: SlotNo -> m ()

    , getBlockInfo :: STM m (HeaderHash blk -> Maybe (BlockInfo blk))

    , filterByPredecessor :: STM m (ChainHash blk -> Set (HeaderHash blk))

    , getMaxSlotNo :: STM m MaxSlotNo
    }
\end{lstlisting}

The database is parameterised over the block type \lstinline!blk! and the monad
\lstinline!m!, like most of the consensus layer.\todo{mention io-sim}
\todo{TODO} Mention our use of records for components?

The \lstinline!closeDB! operation closes the database, allowing all opened
resources, including open file handles, to be released. This is typically only
used when shutting down the entire system. Calling any other operation on an
already-closed database should result in an exception.

The \lstinline!putBlock! operation adds a block to the Volatile DB. There are no
requirements on this block. This operation is idempotent, as duplicate blocks
are ignored.

The \lstinline!getBlockComponent! operation allows reading one or more
components of the block in the database with the given hash. See
\cref{immutable:api:block-component} for a discussion about block components. As
no block with the given hash might be in the Volatile DB, this operation returns
a \lstinline!Maybe!.

The \lstinline!garbageCollect! operation will try to garbage collect all blocks
with a slot number less than the given one. This will be called after copying a
block with the given slot number to the Immutable DB. Note that the condition is
``less than'', not ``less than or equal to'', even though after a block with
slot $s$ has become immutable, any other blocks produced in the same slot $s$
can never be adopted again and can thus safely be garbage collected. Moreover,
the block we have just copied to the Immutable DB will not even be garbage
collected from the Volatile DB (that will be done after copying its successor
and triggering a garbage collection for the successor's slot number).

The reason for ``less than'' is because of EBBs (\cref{ebbs}). An EBB has the
same slot number as its successor. This means that if an EBB has become
immutable, and we were to garbage collected all blocks with a slot less than or
\emph{equal} to its slot number, we would garbage collect its successor block
too, before having copied it to the Immutable DB.

The next two operations, \lstinline!getBlockInfo! and
\lstinline!filterByPredecessor!, allow querying the Volatile DB. Both operations
are \lstinline!STM!-transactions that return a function. This means that they
can both be called in the same transaction to ensure they produce results that
are consistent w.r.t.\ each other.

The \lstinline!getBlockInfo! operation returns a function to look up the
\lstinline!BlockInfo! corresponding to a block's hash. The \lstinline!BlockInfo!
data type is defined as follows:
\begin{lstlisting}
data BlockInfo blk = BlockInfo {
      biHash         :: !(HeaderHash blk)
    , biSlotNo       :: !SlotNo
    , biBlockNo      :: !BlockNo
    , biPrevHash     :: !(ChainHash blk)
    , biIsEBB        :: !IsEBB
    , biHeaderOffset :: !Word16
    , biHeaderSize   :: !Word16
    }
\end{lstlisting}
This is similar to the information stored in the Immutable DB's on-disk indices,
see \cref{immutable:implementation:indices}. However, in this case, the
information has to be retrieved from an in-memory index, as the function
returned from the \lstinline!STM! transaction is pure.

The \lstinline!filterByPredecessor! operation returns a function to look up the
successors of a given \lstinline!ChainHash!. The \lstinline!ChainHash! data type
is defined as follows:\todo{Explain somewhere else and link?}
\begin{lstlisting}
data ChainHash b =
    GenesisHash
  | BlockHash !(HeaderHash b)
\end{lstlisting}
This extends the header hash type with a case for genesis, which is needed to
look up the blocks that fit onto genesis. As the Volatile DB can store multiple
forks, multiple blocks can have the same predecessor, hence a \emph{set} of
header hashes is returned. This mapping is derived from the ``previous hash''
stored in each block's header. Consequently, the set will only contain the
header hashes of blocks that are currently in the Volatile DB. Hence the choice
for the \lstinline!filterByPredecessor! name instead of the slightly misleading
\lstinline!getSuccessors!. This operation can be used to efficiently construct a
path between two blocks in the Volatile DB. Note that only a single access to
the Volatile DB is need to retrieve the function instead of an access \emph{per
lookup}.

The final operation, \lstinline!getMaxSlotNo!, is also an STM query, returning
the highest slot number stored in the Volatile DB so far. The
\lstinline!MaxSlotNo! data type is defined as follows:
\begin{lstlisting}
data MaxSlotNo =
    NoMaxSlotNo
  | MaxSlotNo !SlotNo
\end{lstlisting}
This is used as an optimisation of fragment filtering in the block fetch
client\todo{link}, look up the \lstinline!filterWithMaxSlotNo! function for more
information.

\section{Implementation}
\label{volatile:implementation}

We will now give a high-level overview of our custom implementation of the
Volatile DB that satisfies the requirements and the API.

\begin{itemize}
\item We append each new block, without any extra information before or after
  it, to a file. When $x$ blocks have been appended to the file, the file is
  closed and a new file is created.

  The smaller $x$, the more files are created.\todo{mention downsides} The
  higher $x$, the longer it will take for a block to be garbage collected, as
  explained in \cref{volatile:implementation:gc}. The default value for $x$ is
  currently \num{1000}.

  For each file, we track the following information:
  \begin{lstlisting}
  data FileInfo blk = FileInfo {
      maxSlotNo :: !MaxSlotNo
    , hashes    :: !(Set (HeaderHash blk))
    }
  \end{lstlisting}
  The \lstinline!maxSlotNo! field caches the highest slot number stored in the
  file. To compute the global \lstinline!MaxSlotNo!, we simply take the maximum
  of these \lstinline!maxSlotNo! fields.

\item We \emph{do not flush} any writes to disk, as discussed in the
  introduction of this chapter. This makes writing a block quite cheap: the
  serialised block is copied to an OS buffer, which is then asynchronously
  flushed in the background.

\item Besides tracking some information per file, we also maintain two in-memory
  indices to implement the \lstinline!getBlockInfo! and
  \lstinline!filterByPredecessor! operations.

  The first index, called the \lstinline!ReverseIndex!\footnote{In a sense, this
  is the reverse of the mapping from file to \lstinline!FileInfo!, hence the
  name \lstinline!ReverseIndex!.} is defined as follows:
  \begin{lstlisting}
  type ReverseIndex blk = Map (HeaderHash blk) (InternalBlockInfo blk)

  data InternalBlockInfo blk = InternalBlockInfo {
        ibiFile        :: !FsPath
      , ibiBlockOffset :: !BlockOffset
      , ibiBlockSize   :: !BlockSize
      , ibiBlockInfo   :: !(BlockInfo blk)
      , ibiNestedCtxt  :: !(SomeSecond (NestedCtxt Header) blk)
      }
  \end{lstlisting}
  In addition to the \lstinline!BlockInfo! that \lstinline!getBlockInfo! should
  return, we also store in which file the block is stored, the offset in the
  file, the size of the block, and the nested context (see
  \cref{serialisation:storage:nested-contents}).

  The second index, called the \lstinline!SuccessorsIndex! is defined as
  follows:
  \begin{lstlisting}
  type SuccessorsIndex blk = Map (ChainHash blk) (Set (HeaderHash blk))
  \end{lstlisting}

  Both indices are updated when new blocks are added and when blocks are removed
  due to garbage collection, see \cref{volatile:implementation:gc}.

  The \lstinline!Map! type used is a strict ordered map from the standard
  \lstinline!containers! package. As for any data that is stored as long-lived
  state, we use strict data types to avoid space leaks. We opt for an ordered
  map, i.e., a sized balanced binary tree, instead of a hashing-based map to
  avoid hash collisions. If an attacker manages to feed us blocks that are
  hashed to the same bucket in the hash map, the performance will deteriorate.
  An ordered map is not vulnerable to this type of attack.

\item Besides the mappings we discussed above, the in-memory state of the
  Volatile DB consists of the path, file handle, and offset into the file to
  which new blocks will be appended. We store this state, a pure data type, in a
  \emph{read-append-write lock}, which we discuss in
  \cref{volatile:implementation:rawlock}.

\item To read a block, header, or any other block component from the Volatile
  DB, we obtain read access to the state (see
  \cref{volatile:implementation:rawlock}) and look up the
  \lstinline!InternalBlockInfo! corresponding to the hash in the
  \lstinline!ReverseIndex!. The found \lstinline!InternalBlockInfo! contains the
  file path, the block offset, and the block size, which is all what is needed
  to read the block. To read the header, we can use the file path, the block
  offset, the nested context (see \cref{serialisation:storage:nested-contents}),
  the header offset, and header size. The other block components can also be
  derived from the \lstinline!InternalBlockInfo!.

\item Note that unlike the Immutable DB, the Volatile DB does not maintain CRC32
  checksums of the stored blocks to detect corruption. Instead, after reading a
  block from the Volatile DB and before copying it to the Immutable DB, we
  validate the block using the \lstinline!nodeCheckIntegrity! method, as
  described in \cref{immutable:implementation:recovery}.

\end{itemize}

\subsection{Garbage collection}
\label{volatile:implementation:gc}

\todo{TODO} Sync with \cref{chaindb:gc}.

As mentioned above, when a garbage collection for slot $s$ is triggered, all
blocks with a slot less than $s$ should be removed from the Volatile DB.

For simplicity and following our robust append-only approach, we do not modify
files in-place during garbage collection. Either all the blocks in a file have a
slot number less than $s$ and it can be deleted atomically, or at least one
block has a slot number greater or equal to $s$ and we do \emph{not} delete the
file. Checking whether a file can be garbage collected is simple and happens in
constant time: the \lstinline!maxSlotNo! field of \lstinline!FileInfo! is
compared against $s$.

The default for blocks per file is currently \num{1000}. Let us now calculate
what the effect of this number is on garbage collection. We will call blocks
that with a slot older than $s$ \emph{garbage}. Garbage blocks that can be
deleted because they are in a file only containing garbage are \emph{collected
garbage}. Garbage blocks that cannot yet be deleted because there is a
non-garbage block in the same file are \emph{uncollected garbage}.

The lower the number of blocks per file, the fewer uncollected garbage there
will be, and vice versa. In the extreme case, a single block is stored per file,
resulting in no uncollected garbage, i.e., a garbage collection rate of 100\%.
The downside is that for each new block to add, a new file will have to be
created, which is less efficient than appending to an already open file. It will
also result in lots of tiny files.

The other extreme is to have no bound on the number of blocks per file, which
will result in one single file containing all blocks. This means no garbage will
ever be collected, i.e., a garbage collection rate of 0\%, which is of course
not acceptable.

During normal operation, roughly one block will be added every 20
seconds.\footnote{When using the PBFT consensus protocol (\cref{bft}), exactly
one block will be produced every 20 seconds. However, when using the Praos
consensus protocol (\cref{praos}), on average there will be one block every 20
seconds, but it is natural to have a fork now and then, leading to one or more
extra blocks. For the purposes of this calculation, the difference is
negligible.} The security parameter $k$ used for mainnet is \num{2160}. This
mean that if a linear chain of \num{2161} blocks has been added, the oldest
block has become immutable and can be copied to the Immutable DB, after which it
can be garbage collected. If we assume no delay between copying and garbage
collection, it will take $\num{1000} + \num{2160} = \num{3160}$ blocks before
the first file containing \num{1000} blocks will be garbage collected.

This means that in the above scenario, starting from a Volatile DB containing
$k$ blocks, after every $\mathsf{blocksPerFile}$ new blocks and thus
corresponding garbage collections, $\mathsf{blocksPerFile}$ blocks will be
garbage collected.\todo{expand calculation}

In practice we allow for overlap by delaying the garbage collection, which has
an impact on the effective size of the Volatile DB, which we discuss in
\todo{link ChainDB}.

\subsection{Read-Append-Write lock}
\label{volatile:implementation:rawlock}

We use a \emph{read-append-write} lock to store the state of the Volatile DB.
This is an extension of the more common read-write lock. A RAW lock allows
multiple concurrent readers, at most one appender, which is allowed to run
concurrently with the readers, and at most one writer, which has exclusive
access to the lock.

The \lstinline!getBlockComponent! operation corresponds to \emph{reading}, the
\lstinline!putBlock! operation to \emph{appending}, and the
\lstinline!garbageCollect! operation to \emph{writing}. Adding a new block can
safely happen at the same time as blocks are being read. The new block will be
appended to the current file or a new file will be started. This does not affect
any concurrent reads of other blocks in the Volatile DB. At most one block can
be added at a time, as blocks are appended one-by-one to the current file. To
garbage collect the Volatile DB, we must obtain an exclusive lock on the state,
as we might be deleting a file while trying to read from it at the same time.
During garbage collection, we ignore the current file and will thus never try to
delete it. This means that, strictly speaking, it would be possible to safely
append blocks and garbage collect blocks concurrently. However, for simplicity
(how should the concurrent changes to the indices be resolved?), we did not
pursue this.

As mentioned in \cref{volatile:implementation:gc}, it is often the case that no
files can be garbage collected. As a (premature) optimisation, we first check
(which is cheap) whether any files can be garbage collected before trying to
obtain the corresponding, more expensive lock on the state.

\subsection{Recovery}
\label{volatile:implementation:recovery}

Whenever a file-system operation fails, or a file is missing or corrupted, we
shut down the Volatile DB and consequently the whole system. When this happens,
either the system's file system is no longer reliable (e.g., disk corruption),
manual intervention (e.g., disk is full) is required, or there is a bug in the
system. In all cases, there is no point in trying to continue operating. We shut
down the system and flag the shutdown as \emph{dirty}, triggering a full
validation on the next start-up.

When opening the Volatile DB, the previous in-memory state, including the
indices, is reconstructed based on the on-disk files. The block in each file are
read and deserialised. There are two validation modes: a standard validation and
a full validation. The difference between the two is that during a full
validation, the integrity of each block is verified to detect silent corruption
using the \lstinline!nodeCheckIntegrity! method, as described in
\cref{immutable:implementation:recovery}.

When a block fails to deserialise or it is detected as a corrupt block when the
full validation mode is enabled, the file is truncated to the last valid block
before it. As mentioned at the start of this chapter, it is not crucial to
recover every single block. Therefore, we do not try to deserialise the blocks
after a corrupt one.
