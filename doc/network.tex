\documentclass{report}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath, amssymb, stmaryrd, latexsym, amsthm, mathtools}
\usepackage{mathpazo, times}
\usepackage{float}
\usepackage{listings}
% \usepackage{url}
\usepackage{natbib}
% \usepackage{parskip} % very ugly with lemmas, invariants, etc without intervening text
% \usepackage[disable]{todonotes}
\usepackage{todonotes}
\usepackage{slashed}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows}
\tikzset{
    state/.style={
           rectangle,
           rounded corners,
           draw=black, very thick,
           minimum height=2em,
           inner sep=2pt,
           text centered,
           },
}

\usepackage{forest}
\usepackage{IEEEtrantools}
\usepackage{microtype}
\usepackage{graphicx,color}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor={blue},
  citecolor={blue},
  urlcolor={blue},
  linkbordercolor={},
  citebordercolor={},
  urlbordercolor={}
}
\usepackage[capitalise,noabbrev,nameinlink]{cleveref}

% https://tex.stackexchange.com/questions/132823/ieeetrantools-clash-with-cleveref
\makeatletter
\let\if@IEEEissubequation\iffalse
\makeatother

\usetikzlibrary{arrows}

\newcommand{\coot}[1]{\textcolor{violet}{\emph{#1}}}
\newcommand{\njd}[1]{\textcolor{purple}{\emph{#1}}}
\newcommand{\avieth}[1]{\textcolor{blue}{\emph{#1}}}
\newcommand{\dcoutts}[1]{\textcolor{orange}{\emph{#1}}}
\addtolength{\marginparwidth}{-0.1\marginparwidth}

\newcommand{\var}[1]{\mathit{#1}}
\newcommand{\type}[1]{\mathsf{#1}}
\newcommand{\powerset}[1]{\mathbb{P}(#1)}
\newcommand{\order}[1]{\mathcal{O}\left(#1\right)}
\newcommand{\restrictdom}{\lhd}
\newcommand{\subtractdom}{\mathbin{\slashed{\restrictdom}}}
\newcommand{\restrictrange}{\rhd}

% add reference to the relevant Haskell source (as comment)
%\newcommand{\hsref}[1]{}  should be hidden in the final document
\newcommand{\hsref}[1]{\href{https://github.com/input-output-hk/ouroboros-network/blob/master/#1}{\emph{Haskell source: #1}}}
\newcommand{\wip}[1]{\color{magenta}{#1}\color{black}}
\newcommand{\hide}[1]{}
\newcommand{\trans}[1]{\texttt{#1}}
\newcommand{\state}[1]{\texttt{#1}}
\newcommand{\msg}[1]{\texttt{#1}}
\newcommand{\Idle}{\state{Idle}}
\newcommand{\Busy}{\state{Busy}}
\newcommand{\Done}{\state{Done}}
\newcommand{\MsgDone}{\msg{MsgDone}}

% TODO: the document is using `\langle` and `\rangle` to denote lists, maybe
% it's better to use Haskell notation, will it be more in sync with other docs
% produced by the formal method team?
\renewcommand{\langle}{[}
\renewcommand{\rangle}{]}

\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\minimum}{min}
\DeclareMathOperator*{\maximum}{max}

% Number within sections, and don't have separate counters for separate environments
\theoremstyle{definition}{
  \newtheorem{lemma}{Lemma}[section] % Number within sections
  \newtheorem{definition}[lemma]{Definition}
}
\theoremstyle{theorem}{
  \newtheorem{invariant}[lemma]{Invariant}
  \newtheorem{proofobligation}[lemma]{Proof Obligation}
}

\Crefname{invariant}{Invariant}{Invariants}

\numberwithin{equation}{lemma}

%\floatstyle{boxed}
%\restylefloat{figure}

\lstset{basicstyle=\ttfamily\small}

\raggedbottom

\begin{document}

\title{Data Diffusion and Peer Networking in Shelley\\
       {\small (Version 0.9)} \\
       {\large \sc An IOHK technical report}}
\author{Duncan Coutts \\ {\small \texttt{duncan@well-typed.com}} \\
                         {\small \texttt{duncan.coutts@iohk.io}}
   \and Alex Vieth \\ {\small \texttt{alex@well-typed.com}}
   \and Neil Davies \\ {\small \texttt{neil.davies@pnsol.com}} \\
                       {\small \texttt{neil.davies@iohk.io}}
   \and Marcin Szamotulski \\ {\small \texttt{marcin.szamotulski@iohk.io}}
   \and Karl Knutsson \\ {\small \texttt{karl.knutsson@iohk.io}}
   \and Marc Fontaine \\ {\small \texttt{marc.fontaine@iohk.io}}
   }
%\date{January 25 2019}
\date{\today}

\maketitle

\begin{abstract}
  This document describes .....
\end{abstract}

\tableofcontents

\section*{Version history}

\begin{description}
\item[Version 0.1, Dec 20, 2018  Draft of the table of contents.]
\item[Version 0.2, Jan 08, 2019 Structure and outline.]
\item[Version 0.3, Jan 25, 2019 Start elaborating contents.]
  Description of state machines; Binary representation.
\item[Version 0.4, Jan 28, 2019 Extend state machine description]
\item[Version 0.5, Feb 1,  2019  add PingPong and Block-Fetch protocol]
\item[Version 0.6, Feb 18, 2019  Overview sections and gluing parts]
\item[Version 0.7, Feb 25, 2019 ]
  complete ChainSync Protocol;Request Response Protocol;
\item[Version 0.8, Mar 11, 2019 ]
  complete Transaction Submission Protocol
\end{description}

\chapter{Overview}
Ouroboros is a family of new protocols for a proof of stake crypto currency.
The Ouroboros papers describe these protocols in a high-level mathematical formalism
that allows for a concise presentation and is appropriate for peer review by cryptography experts.
For separation of concerns, the papers do not describe a concrete implementation of the Ouroboros
protocols.

This document has a broader scope.
It addresses system designers and engineer who want to implement Ouroboros or understand
the references implementation and who have an IT background but are not necessarily crypto experts. 
The description of the protocol in this document contains
all information that is needed to implement a compatible network node.
It covers, how nodes join the network, the binary format of the messages that nodes exchange and
the order in which nodes send and receive the messages of the protocol.
This is the information that is typically found in the description of network protocols.

However, the Ouroboros proof of stake crypto currency has additional requirements,
that the nodes and the network must meet, and, that are not typically covered in a
protocol  description itself.
These underlying requirements are essential for understanding the design of the protocol.
Although, not necessarily needed for a bare protocol description,
it makes sense to also discuss these aspects and requirements in this document.

Shelley is a first fully decentralised release of Ouroboros protocol.

This Chapter contains an overview of the content and scope of this document and aspects that
are being discussed.

\subparagraph{Software assurance}
\wip{Software assurance}

\subparagraph{Layered Protocols}
Traditionally network protocols are presented as a stack of layers where
upper layers build on services provided by lower layers and lower layers
are agnostic of the implementation of the upper layers.
This concept of layers is misleading when discussing Ouroboros.
It is {\em not} the case that the consensus (layer) is build on top of the network (layer).
It is more appropriate to talk about a network component than a network layer.
The network component provides services to the consensus component and vice versa.
Both components rely on each other.
The network component uses the consensus component validate
information that it distributes to other nodes.
This is essential to guard against certain kinds of DoS attacks.
Existing peer-to-peer systems focus on a slightly different problem domain.
For example, they do not consider the information validation issue
or have worries \todo{(Neil : I could not read the word)}, such as 'eclipse' that to not
apply to the Ouroboros family of protocols.

\missingfigure[figwidth=6cm]{Testing a long text string}

\subparagraph{Performance of Peer to Peer Networks}
In computer science, Byzantine Fault Tolerance is a property which states
that a distributed algorithm works for the honest participants of the algorithm
under the assumption that a certain percentage of the participants is indeed honest.
A similar, but more informal property applies to peer to peer networks as well.

A peer to peer network, is a system which provides a service to its participants while at the same
time the participants of the network provide a service to the network.
The performance of the P2P network depends (among other things) on the performance of the nodes,
while the performance of a node also depends on the performance of the network.
Not only are there honest and adversarial participant, but there is also a huge variety of
possible network topographies, bandwidths and latencies of network connections and other
factors that determine the performance of the network.
This document discusses the high level business requirements for Cardano and what are the
assumptions on the structure of the underlying P2P network.

\subparagraph{Protocol vs Implementation}
Network Protocols are written on different levels of abstractions.
To be useful a protocol must be precise enough to be implemented.
At the same time a protocol should not only be precise but also abstract.
A high level of abstraction allows different alternative implementations of the protocol
and also makes it possible to develop and improve it.
Furthermore, it should not only, in principle, be possible to implement an
abstract version of a protocol
but also faithfully interpret it in a real world scenario.
For example, it must be possible to implement a protocol
such that the real world software runs on a real world machine with a common size of main memory
and a common speed network adaptor.
The Shelley network protocol has been developed in parallel with a reference implementation in Haskell.
Haskell (more precisely GHC) has built-in support for high level concurrency abstractions
like, for example, light-weight threads and software transactional memory.
While the protocol itself is completely language agnostic, it still makes sense to discuss some
aspects of the Haskell reference implementation in this document.
In particular, the document describes how to achieve good resource bound, for
a protocol implementation.

\subparagraph{Threats}
\wip{Reference the Threats section.
  'eclipse' can be deterred}


\wip{TODO:extended abstract, scope of the document}

\section{High level requirements}

These are the high level business requirements for the networking that were
gathered and signed off in late 2017. As such they are expressed in an informal
prose style, and often following a ``user story'' style.
Roughly there are three different kinds of users:
\begin{itemize}
\item Users who have delegated.
\item Small stakeholders.
\item Large stakeholders.
\end{itemize}

\subsubsection{Network connectivity}\label{network-connectivity}

\paragraph{Participate as a user who has delegated}

As a Daedalus home user with my stake delegated to other users I would like to
join the Cardano network so I can participate in the network.
\begin{itemize}
\item The system be designed to provide this user segment with the ability
      to catch up and keep up with the blockchain without having to do any local
      network configuration.
\item The system be designed to provide this user segment with the ability to
      continuously find and maintain a discovery of a sufficient number of
      other network participants that have reasonable connectivity.
\item The system be designed to provide this user segment with the ability to
      find and maintain a minimum of 3 other network participants to maintain
      connectivity with performance that is sufficient to catch up with the
      blockchain.
\item The system design will take into account that this user will probably be
      behind a firewall.
\item Users in the segment can be defined by as having all their stake
      delegated to other network participants.
\end{itemize}


\paragraph{Participate in network as small stakeholder}

As a Daedalus home user operating a core node with a small stake, I would like
to join the Cardano network so I can participate in the network as a core node
that produces blocks i.e. have not delegated to someone else.

\begin{itemize}
\item The system be designed to provide this user segment with the ability to
      receive the transactions that will be incorporated into blocks (although
      sizing the operation of the distributed system to ensure that all such
      participants would be able to receive all transactions is not a bounding
      constraint).
\item The system be designed to provide this user segment with the ability to
      participate in the MPC protocol\footnote{This requirement is now
      redundant because the MPC protocol is specific to Ouroboros Classic.}.
\item The system will be designed to provide this user segment with the ability
      to catch up and keep up with the blockchain without having to do any local
      network configuration (this is a bounding constraint).
\item The user will have sufficient connectivity and performance to receive a
      block within a time slot {\sc and} they have to be able to create and
      broadcast a block within a time slot in which the block is received by
      other core nodes.
\item The system will be designed to maximise the likelihood that 50\% of home
      users operating a core node are compliant with the previous requirement
      at any one time.
\item The system will be designed to provide this user segment with the ability
      to continuously find and maintain a discovery of a sufficient number of
      other network participants that have reasonable connectivity.
\item The system will provide a discovery mechanism that will find and maintain
      a minimum of 3 other network participants to maintain connectivity with
      performance that is sufficient to catch up with the blockchain.
\item The system design will take into account that this user may be behind a
      firewall (i.e being behind a firewall should not preclude a user
      participating in this fashion).
\item The Delegation workstream will provide a UI feature for the user to
      choose to control their own stake.
\item Users in this segment will be defined as {\sc not}
      \begin{itemize}
      \item[a)] being in the top 100 users ranked by stake or
      \item[b)] in a ranked set of users who combined control 80\% of the stake
      \end{itemize}
\item Users in this segment will not be part of the Core Diff, but still
      subject to the normal incentives related to creating blocks.
\end{itemize}


\paragraph{Participate in network as a large stakeholder}

As a user running a core node on a server and with large stake in the network,
I would like to join the Cardano network so I can participate in the network as
a core server node that produces blocks i.e. have not delegated to someone else.

\begin{itemize}
\item A large stakeholder will be defined as
      \begin{itemize}
      \item[a)] being in the top 100 users ranked by stake; or
      \item[b)] in a ranked set of users who combined control 80\% of the stake
      \end{itemize}
\item Assuming that this user has sufficient connectivity and performance, the
      system should ensure that the collective operation of the distributed
      system will ensure that they have a high probability, of receiving a
      block within a time slot such that they have sufficient time to be able
      to create and broadcast a block within a time slot where the block is
      received by other core nodes.
\item It is expected that the previous requirement will be fulfilled to a high
      degree of reliability between nodes in this category -- assuming normal
      network operations

      \begin{tabular}{rl}
      Threshold & $>95\%$ \\
      Target    & $>98\%$ \\
      Stretch   & $>99\%$
      \end{tabular}
\item The system will be designed to provide this user segment with the ability
      to continuously find and maintain a discovery of a sufficient number of
      other network participants that have reasonable connectivity.
\item Discovery will find and maintain a minimum of 10 other network
      participants to maintain connectivity with performance that is sufficient
      to catch up with the blockchain.
\item Ability to receive the transactions that will be incorporated into blocks.
\item Ability to participate in the MPC protocol\footnote{This requirement is
      now redundant because the MPC protocol is specific to Ouroboros Classic.}.
\item The user will catch up and keep up with the blockchain.
\item The server firewall rules will be such that it can communicate with other
      core nodes on the system (and vice versa) -- The system will provide the
      necessary information to update firewall rules if the server is operating
      behind a firewall to ensure the server can communicate with other core
      nodes.
\item The threshold which defines the group of large stakeholders may be
      configurable on the network layer. The configuration may include toggling
      between the rules a) and b) in the previous requirement and the threshold
      numbers within these (this is pending a decision from the Incentives
      workstream.
\item The rules and threshold configuration may need to be a protocol parameter
      that is updated by the update system.
\end{itemize}


\paragraph{Poor network connectivity notification}

As a home user, I want to see a network connection status on Daedalus so that
I know the state of my network connection.
%
\begin{itemize}
\item If the user receives a notification that they are in red or amber mode,
      Daedalus will give the user some helpful information on how to resolve
      common connectivity issues.
\end{itemize}
%
There are three (at least) the following three distinct modes that the network can be operating in:
each one has a red, green, amber status.

%
\begin{center}
\begin{tabular}{ll}
Initial block sync \\
\hline
red   & receiving $<1$ blocks per 10s \\
amber & receiving $<10$ blocks per 10s \\
green & otherwise  \\[1em]

Recovery \\
\hline
red   & receiving $<1$ slot block per 10s \\
amber & otherwise  \\
green & (not applicable) \\[1em]

Block chain following \\
\hline
red   & it has been more that 200s since a slot indication was received. \\
amber & it has been more than 60s since a slot indication was received. \\
green & otherwise.
\end{tabular}
\end{center}

This assumes that the slot time remains 20 seconds.


\subsubsection{Distributed System Performance}

\paragraph{Transaction Throughput}

The transaction per second of the system as a whole will be:
%
\begin{center}
\begin{tabular}{lr}
Threshold &  8 tps  \\
Target    & 16 tps  \\
Stretch   & 50 tps  \\
\end{tabular}
\end{center}

This assumes that the slot time remains 20 seconds.

\paragraph{Transaction Latency}

As a user I want my transaction to be submitted to the blockchain and received
by the target user within the following time period:
%
\begin{center}
\begin{tabular}{lr}
Threshold & 100 seconds \\
Target    & 60 seconds  \\
Stretch   & 30 seconds  \\
\end{tabular}
\end{center}
%
The above time-frames will be achieved for $>95\%$ of all transactions.

\paragraph{Network Bearer Resource Use -- end user control}

As a user operating on the network as a home user not behind a firewall, I
would like a cap on the total amount of network capacity in terms of short-term
bandwidth that other network users can request from my connection so I am
assured my network resource is not eaten up by the data diffusion function.

\begin{itemize}
\item The cap should be based on a fraction of a typical home internet
      connection -- it can be changed by configuration including ``don't act
      as a super node''.
\item The system will allow users syncing with the latest version of the
      blockchain to download blocks from more then one and up to five network
      peers concurrently.
\item A cap on number of incoming subscribers.
\item A cap on number of outbound requests for block syncing from other users.
\item The cap will not be imposed on core nodes running on a server.
\item If these resources are available, a reasonable connection speed should be
      available to users requesting to sync the latest version of the
      blockchain e.g. downloading blocks from 5 peers concurrently to aggregate
      the bandwidth.
\item (nice to have) the actual number and capacity being used is available to
      user.
\end{itemize}

\paragraph{Participant performance measurement}

There may be a requirement for measuring if a large stakeholder is not meeting
their network obligation.

It is accepted that this requirement is a ``nice to have'', and it has not been
established that it is possible, nor has it been incorporated into the
incentives mechanism.

\subsubsection{Distributed System Resilience and Security}

\paragraph{Resilience to abuse}

As a user I should not be able to attack the system using an asymmetric denial
of service attack that will deplete network resources from other users.

\begin{itemize}
\item The system should achieve its connectivity and performance requirements
      even in the presence of a non-trivial proportion of bad actors on the
      network.
\item There is an assumption that there are not a large numbers of bad actors
      in the network.
\item The previous assumption does not follow from the assumptions of Ouroboros
      which states that the users that control 50\% of the stake are
      non-adversarial.
\end{itemize}


\paragraph{DDoS protection}

As a large stakeholder running a core node on a server, I should still be able
to communicate with other user in this segment, even if the system comes under
a DDoS attack.

\begin{itemize}
\item Users in this segment will be able to generate and broadcast blocks to
      each other within the usual timing constraints in this situation.
\end{itemize}

IP addresses will be hidden

\begin{itemize}
\item Encrypted IP addresses will be published by 10 of the other members of
      the group of large stakeholder core nodes.
\end{itemize}

Assumption

\begin{itemize}
\item Core node operators will not publish their IP addresses publicly.
\item Encrypted IP addresses will be published by the 10 of the other members
      of the group of large stakeholder core nodes.
\item If a node operators IP address is compromised the operator will respond
      and change the IP address of their node.
\item The system will allow operators to change the address of their core nodes
      and communicate with that new IP address within a reasonable period of
      time.
\end{itemize}


\subsubsection{Network decentralisation's}

\paragraph{No hegemony}

As a user I want to be assured that IOHK and its business partners are not in
an especially privileged position in terms of trust, responsibility and
necessity to the network so that network hegemony is avoided.

\begin{itemize}
\item IOHK should be in the same position on the network as any other
      stakeholder with an equivalent amount of stake.
\item There is more general requirement that no other actor could do this
      either.
\end{itemize}

\section{Context and Introduction}

How this work relates (in general terms) to the rest of the Shelley
development and the Ouroboros papers.

\subsection{Data Diffusion assumptions in Ouroboros}
\begin{quote}
  This is the ``telling them what you are going to tell them'' part -
  outline of the data diffusion and overlay network bringing out the
  key functional and non-functional relationships - aim to serve as a
  general executive overview as well as a framing of PoS issues.
\end{quote}
\begin{itemize}
  \item Assumptions of the mathematical model(s)
  \item Goal of the data diffusion functionality
  \item Strong requirement on collective performance
  \begin{itemize}
    \item Chain growth quality
    \item Adversarial actor assumption
  \end{itemize}
\end{itemize}
\subsection{Functional Layering}
\todo[inline]{pictures as to how the various functional layers relate; How the Data
diffusion layer relates to Ledger etc; How data diffusion relates to
point-to-point overlay network.}

\subsubsection{Non-function aspects}
Performance; trustworthiness; Forwarding as an expression of confirmed
``trust'' (and corollary - forwarding of \emph{clearly} incorrect information
seen as prima-facia evidence of adversarial action.
\subsection{Protocol Roles}
Peer relationship between various nodes; Limited trust and
verification; (Brief) description of the expectations and assumptions across
the functionality boundaries; 

\subsubsection{Mini Protocols}
\begin{description}
\item[Chain Sync]
\item[Block Fetch]
\item[Transaction Submission]
\item[$\Delta Q$ Measurement] (not really a mini protocol, in that it
  is point to point and not part of the diffusion process itself,
  placed here because it ``sits'' on top of the overlay network. Role
  to generate active endpoint performance data to help optimise
  time-to-diffuse critical information exchanges (e.g. newly minted
  block diffusion)
\end{description}  
\subsubsection{Point to point Overlay Network}
\todo[inline]{need a picture} Note that long term eclipse attacks are not an issue
here (cover why a bit later); tie in chain growth requirements with
need for ``better'' communications performance between (major)
stake pools (notion of Core DIF); association explain need for
\begin{itemize}
\item fixed configuration (with/without others from this list)
\item Core DIF
\item Distributed endpoint discovery (subset of notion of peer in
  other approaches)
\end{itemize}

  
\section{Layout of the Document}

\begin{itemize}
  \item What goes in which section ?
  \item In which order to read ?
  \item Which sections can be skipped ?
\end{itemize}

\section{Notation}

\chapter{Requirements}

\section{Performance Requirements and User Stories}
\subsection{Classes of Participants}
\wip{todo: make a nice table}
\todo{Some information about participants is in the previous
section~\ref{network-connectivity}}

\subsubsection{Stake pool} % lookup what this is called in the protocols.tex
\subsubsection{Small stakeholder}
\subsubsection{User who has delegated}
\subsubsection{Requirements for Participants}
\subsubsection{Requirements for Stake Pools}
\subsubsection{Services that the System should provide}

There are two kinds of Requirements:

\begin{enumerate}
\item System capabilities for a node to take a blockchain slot creation role in the protocol.
\item What services that the system provides to the user.
\end{enumerate}



\section{Protocol Updates on the Blockchain}
\begin{itemize}
\item Hybrid phase of federation and decentralisation
\item Gradually transition between protocol variants on a live blockchain.
\item Several protocol variants active in parallel, version negotiation.
\item Communication between Shelley Nodes and existing core nodes.
\item Byron proxy
\end{itemize}

\section{Node to Node and Node to Consumer IPC}
There are two basic variants of inter-process-communication in the network:
\begin{itemize}
\item IPC between Cardano nodes that are engaged in the high level Ouroboros
      blockchain consensus protocol.
\item IPC between a Cardano node and a `chain consumer' component such as a
      wallet, explorer or other custom application.
\end{itemize}
Both variants of IPC in the network follow distinct requirements and constraints, and
,while the first version of Cardano used a single protocol, the new version will
use different sets of protocols for both uses cases.
(See Section~\ref{why_distinguish_protocols} for the motivation for this design decision.)
Throughout the document it will be clear which variant of we are referring to.

\section{Threat Model}
\wip{
  WIP discussion of the threat model
\subsection{Resource Consumption Attacks}
}

\section{Ouroboros}
\wip{
How PoS is different from PoW in its network requirements:
}

\begin{itemize}
  \item No capability to sustain an undetected Eclipse attack
  \item Sustained liveness requirement
\end{itemize}

\section{Delegation}

\chapter{System Architecture}
\section{Overview}
This section describes the key underlying aspects and considerations of the design
of the network protocol and tries to give a big picture the overall network protocol.

\section{Key Functionality}

\wip{
  Here only an outline. Full discussion in the Section~\ref{ouroboros}.
\begin{description}
\item[Block diffusion]
\item[Chain following]
\item[Forwarding transactions]
\end{description}
}

\section{High Assurance Software}
\wip{How does the software/protocol design reflect the high assurance requirements ?}

\section{Modular Design and Mini Protocols}
\wip{Why are mini protocols a good idea ?}
\wip{How does polymorphism help modularity ?}

\section{Congestion Control}
A central design goal of the system is robust operation at high work loads.
For example, it is a normal working condition of the P2P network
that transactions arrive at a higher rate than the number of transaction
that can be included in block chain.
An increase of the rate at which transactions arrive at the network must not cause a decrease
of the block chain quality.

The TCP network layer itself does not deal well with over-loading.
A TCP connection has a certain maximal bandwidth,
i.e. a certain maximum load that it can relatively reliably handle under normal conditions,
but, if the connection is ever overloaded, the throughput will
drastically break down and the connection will typically not recover by
itself.  This means that the node must actively control the load it puts on
its network connections.

At the same time, the node itself has a limit on the rate, at which it can process data.
In particular, a node will have to share its processing power with other processes that run on the
same machine/operation system instance, which means, that a node may get slowed down for some reason,
and the system may get in a situation,
where there is more data available from the network than the node can process.
The design must guarantee that the node still works properly in this situation.
In any condition, a node must not exceed its memory limits.
(It must not buffer incoming data that cannot be processed in time.)

Of cause it makes no sense if the system design is robust, but so defensive that it wastes on the possible
performance.
An example would be a protocol that never transmits a messages unless it has received and
explicit ACK for the previous message. This approach might be safe of overloading the network, but,
on the other hand it also waste most of the bandwidth that is potentially available.

\subsection{Back Pressure}
Back Pressure is a strategy for congestion control for push based buffered channels like
,for example, TCP connections.



\wip{TODO: explain the concept of Back pressure and how it is used in the design}
\subsection{Push vs Pull}
\wip{TODO: data flow inside the node is pull based}
\subsection{Delta Q}
\wip{TODO: explain what delta Q means and what it has to do with congestion control.}

\begin{figure}[h]
  \pgfdeclareimage[height=10cm]{node-diagram-chains-state}{node-diagram-concurrency.pdf}
  \begin{center}
    \pgfuseimage{node-diagram-chains-state}
  \end{center}
  \caption{Data flow inside a Node}
  \label{node-diagram-concurrency}
\end{figure}


\section{Data Flow in a Node}
Nodes maintain connections with the peers that have been chosen with help of the peer selection process.
Suppose node $A$ is connected to node $B$.
The Ouroboros protocol schedules a node $N$ to generate a new block in a given time slot.
Depending on the location of nodes $A$, $B$ and $N$ in the network topology and whether the new
block arrives first at $A$ or $B$ ($A$ can be either up-stream or down-stream of $B$).
Therefore, node $A$ runs an instance of the client side of the chain-sync mini protocol
that talks with a server instance of chain-sync at node $B$ and also a server instance of chain sync
that talks with a client instance at $B$.
The situation is similar for the other mini protocols (block fetch, transaction submission, etc).
The set of mini protocols that runs over a connection is determined by version of the network
protocol, i.e.  Node-to-Node, Node-to-Wallet and Node-to-Chain-Consumer
connections use different sets of mini protocols (e.g. different protocol
versions).  The version is negotiated when a new connection is established
using protocol which is described in Chapter~\ref{connection-management}.
\todo{Add description of this protocol in Chapter~\ref{connection-management}
and link it.}

Figure~\ref{node-diagram-concurrency} illustrates parts of the data flow in a node.
Circles represents a thread that runs one of the mini protocols (the mini protocols are explained in
Chapter~\ref{state-machine-section}).
There are two kinds of data flows involved.
Mini protocols communicate with mini protocols of other nodes by sending and receiving messages,
and, within a node, they communicate by reading from- and writing to- a shared
mutable state (represented by boxes in Figure~\ref{node-diagram-concurrency}).
\href{https://en.wikipedia.org/wiki/Software_transactional_memory}{Software transactional memory} (STM) is a mechanism for safe and lock-free concurrent
access to mutable state and the reference implementation makes heavy use of it.

\section{Real-time Constraints and Coordinated Universal Time}
Ouroboros splits the physical time into an infinite sequence of time slots,
i.e. contiguous, equal-length intervals of time
and assigns the slot leaders, i.e. the nodes that are eligible to create a new block, to those time slots.
At the beginning of a time slot, the slot leader selects the block chain and transactions that are the basis
for the new block, then it creates the new block and sends the new block to its peers.
If the new block reaches the next block leader before the beginning of next time slot,
the next block leader can extend the block chain upon this block (if the block
did not arrive on time the next leader will create a new block regardless of
the missing block).

There are some trade-offs when choosing the slot time that is used for the protocol but
basically the slot length should be long enough such that a new block has a good chance to reach the
next slot leader in time.
A typical value for the slot length is 20 seconds.
It is assumed that the clock skews between the local clocks of the nodes is small with respect to the
slot length.

However, no matter how accurate the local clocks of the nodes are with respect to the time slots
the effects of a possible clock skew must still be carefully considered.
For example, when a node time-stamps incoming blocks with its local clock time, it may encounter
blocks that are created in the future
(with respect to the local clock of the node).
The node must then decide whether this is because of a clock skew or whether the node considers this
as adversarial behaviour of an other node.

\wip{TODO :: get feedback from the researchers on this. Tentative policy: allow 200ms to 1s
explain the problem in detail.
A node cannot forward a block from the future.
This is complicated !
}

\chapter{Mini Protocols}
\label{state-machine-section}

\section{Mini Protocols and Protocol Families}
A mini protocol is a well defined and modular building block of
the network protocol.
Structuring the protocol around mini protocols helps to manage the overall complexity of
the design and adds useful flexibility.
The design turns into a family of protocols that can be specialised to particular requirements
by choosing a particular set of mini protocols.

The mini protocols in this section describe both the initiator and responder of a communication.
The initiator is the dual of the responder and vice versa.
(The terms client/server and consumer/producer are also used sometimes.)
At any time a node will typically run many instances of mini protocols, including many instances of the
same mini protocol.
Each mini protocol instance of the node communicates with the dual
instance of exactly one peer.
All mini protocols, that communicate with the same peer,
share a single communication channel (pipe or socket)
and a multiplexer/de-multiplexer is used to multiplex the protocols over that channel.
Section~\ref{multiplexing-section} describes the multiplexing layer.

The set of mini protocols, that run on a connection between two participants of the system,
depends on the role of the participants, i.e. whether the nodes acts as a full node or just
a block chain consumer, for example a wallet.
Section~\ref{peer-setup-section} describes how a connection between two nodes,
that run a set of mini protocols, is set up.

\section{Protocols as State Machines}
The reference implementation of several mini protocols uses a generic framework
for state machines.

The Haskell implementation of the state machine framework is described in
Section~\ref{Haskell-state-machine}. It uses correct-by-construction
techniques to guarantee
several properties of the protocol and the implementation.
In particular, it guarantees that there are no deadlocks, i.e., at any time, one side has agency
(is expected to transmit the next message) and the other side is awaiting for
the message (or both sides agree that the protocol has terminated).
If either side receives a message that is not expected according to the protocol
the communication is aborted.

For each mini protocol, that is based on this underlying framework, the description provides the
following pieces of information:

\begin{itemize}
\item An informal description of the protocol.
\item States of the state machine
\item The messages that are exchanged
\item A transition graph of the global view of the state machine.
\item The client implementation of the protocol.
\item The server implementation of the protocol.
\end{itemize}

\begin{description}
\item[State Machine]
  Each mini protocol is described as a state machine.
  This document uses a simple diagram representations for state machines, we
  also include corresponding transition tables.
  Description of a state machines in this section are directly derived from
  specification of a mini protocol using the state machine framework.

  The state machine framework, that is used to specify the protocol, can be instantiated
  with different implementations that work at different levels of abstraction
  (for example implementations used for simulation, implementations that run over virtual
  connections and implementations that actually transmit messages over the real networks).
  The exact interpretation of the states and the transitions of the state machine
  depends on the actual implementation.

\item[States]
  State are abstract, they are not a value of some variables in a node, but
  rather describe the state of the two party communication as whole, e.g.
  that a client is responsible for sending a particular type of message and
  the server is awaiting on it.  This, in particular, means that if the state
  machine is in a given state, both client and server are in this state.
  Additional pice of information that differentiates the roles of peers in
  a given state is agency, which describes which side is responsible for
  sending the next message.

  In the state machine framework, abstract states of a state machine are
  modelled as promoted types (so they do not correspond to any particular
  value hold by one of the peers).

  The document presents this abstract view of mini protocols and the state
  machines where the client and server are always in identical states, which
  also means that client and server simultaneously transit to new states.
  For this description network delays are not important.

  An interpretation, which is closer to the real-world implementation (but
  less concise), is that there are independent client and server states
  and that transitions on either side happens independently when a message is send or received.
  
\item[Messages]
  Messages exchanged by peers form edges of a state machine diagram, in other
  words they are transitions between states.
  They are elements from the set
  $$\{(label, data) \mid label \in Labels, data \in Data\}$$
  Protocols use a small set of $Labels$ typically $|Labels| \leq 10$.
  The state machine framework requires that messages can be serialised,
  transferred over the network and de-serialised by the receiver.
  The binary format for messages is described in Section~\ref{CBOR-section}.

\item[Agency]
  A node has agency if it is expected to send the next message.
  In every state (except the \Done-state) either the client or server has agency
  In the \Done-state the protocol has terminated and neither side is expected to send any more
  messages.

\item [State machine diagrams]
      States are drawn as circles in state machine diagrams.
      States with agency at the client are drawn in green, states with agency at the server in blue and
      the \Done-state in black.
      By construction, the system is always in exactly one state,
      i.e. client's state is always the same state as server's,
      and the colour indicates who is the agent.
      It is also important to understand, that the arrows in the state transition diagram denote
      state transitions and not the direction of the message that is being transmitted.
      For the agent of the particular state it will mean send a message to the
      other peer, for a non-agent, listen for incoming message.
      This may be confusing because the arrows are labeled with the messages and
      many arrows go from a green state (client has the agency) to a blue
      state (server has the agency) or vice versa.

%TODO: find a latex style to mark tricky sections that must be read carefully.

\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4.5cm, semithick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]
  \node[state, green]              (A)      {$A$};
  \node[state, blue ,right of=A]   (B)      {$B$};
  \draw (A)            edge[above]          node{Message}   (B);
\end{tikzpicture}

      $A$ is green, i.e in state $A$ the client has agency.
      Therefore the client sends a message to the server and
      both client and server transition to state $B$.
      As $B$ is blue the agency also changes from client so server.

\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4.5cm, semithick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]
  \node[state, blue]               (C)      {$C$};
  \node[state, blue ,right of=A]   (D)      {$D$};
  \draw (A)            edge[above]               node{Message}   (B);
\end{tikzpicture}

      $C$ is blue, i.e in state $C$ the server has agency.
      Therefore the server sends a message to the client and
      both client and server transition to state $D$.
      As $D$ is also blue the agency remains at the server.

\item[Client and server implementation]
  The state machine describes which messages are send and received and in which order.
  This is the external view of the protocol that every compatible implementation MUST follow.
  In addition to the external view of the protocol, this part of the specification describes
  how the client and server actually process the transmitted messages,
  i.e. how the client and server update their internal mutable state
  upon the transmission of messages.

  Strictly speaking, the representation of the node-local mutable state
  and the updates to the node-local state, are implementation details that are
  not part of the communication protocol between the nodes,  and it will
  depend on an application that is build on top of the network service
  (wallet, core node, explorer, etc.).
  The corresponding sections were added to clarify mode of operation of the
  mini protocols.

\end{description}

\section{Ping-Pong Protocol}
\label{ping-pong-protocol}
\hsref{typed-protocols/src/Network/TypedProtocol/PingPong/Type.hs}
\newcommand{\Ping}{\msg{Ping}}
\newcommand{\Pong}{\msg{Pong}}

\subsection{Description}
A client can use the Ping-Pong protocol to check that the server is responsive.
The Ping-Pong protocol is very simple, because the messages do not carry any data and
because the Ping-Pong client and the Ping-Pong server do not access the internal state of the node.
It uses the same framework for state machines as the other protocols,
but because the protocol is so simple,
the description of the protocol is also very simple and slightly
different from descriptions of other protocols.
None the less the reader might
find it useful to get familiar with the protocol description we present in this report.

\subsection{State Machine}

\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4.5cm, semithick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]
  \node[state, green, initial]                            (Idle)      {\Idle};
  \node[state, right of=Idle]                             (Done)      {\Done};
  \node[state, blue, below of=Idle]                       (Busy)      {\Busy};

  \draw (Idle)         edge[above]               node{\MsgDone}                  (Done);
  \draw (Idle)         edge[left, bend right]    node{\Ping}                     (Busy);
  \draw (Busy)         edge[right, bend right]   node{\Pong}                     (Idle);
\end{tikzpicture}

\begin{figure}[H]
\begin{tabular}{|l|l|} \hline
\multicolumn{2}{|c|}{Agency} \\ \hline
  Client has Agency & \Idle \\  \hline
  Server has Agency & \Busy \\  \hline
\end{tabular}
\end{figure}

The protocol uses the following messages.
The messages of the Ping-Pong protocol do not carry any data.
\begin{description}
\item [\Ping]
      The client sends a Ping request to the server.
\item [\Pong]
      The server replies to a Ping with a Pong.
\item [\MsgDone]
      Terminate the protocol.
\end{description}

\begin{tabular}{|l|l|l|}
  \hline
  \multicolumn{3}{|c|}{Transition table} \\ \hline
  from state   & message            & to state    \\ \hline\hline
  \Idle        & \Ping              & \Busy   \\ \hline
  \Busy        & \Pong              & \Idle   \\ \hline
  \Idle        & \MsgDone           & \Done       \\ \hline
\end{tabular}

\section{Request Response Protocol}
\label{request-response-protocol}
\renewcommand{\Idle}{\state{Idle}}
\renewcommand{\Busy}{\state{Busy}}
\renewcommand{\Done}{\state{Done}}
\newcommand{\Request}{\msg{Request}}
\newcommand{\Response}{\msg{Response}}

\subsection{Description}
The request response protocol is polymorphic in the request and response data that is being transmitted.
This means that there are different possible applications of this protocol and the
application of the protocol determines the type of the requests and responses.

This is just an example protocol which does not need to be supported, but it
may be useful in the future.  It's also useful as a simple example of
a protocol description.

\subsection{State machine}
\hsref{ouroboros-network/src/Ouroboros/Network/Protocol/ReqResp/Type.hs}

\begin{tabular}{|l|l|}
  \hline
  \multicolumn{2}{|c|}{Agency} \\ \hline
  Client has Agency & \Idle \\  \hline
  Server has Agency & \Busy \\ \hline
\end{tabular}

\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4.5cm, semithick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]
  \node[state, green, initial]                                   (Idle)      {\Idle};
  \node[state, blue, right of=Idle]                              (Busy)      {\Busy};
  \node[state, right of=Busy]                                    (Done)      {\Done};

  \draw (Idle)         edge[]      node{\Request}           (Busy);
  \draw (Busy)         edge[]      node{\Response}          (Done);
\end{tikzpicture}

\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{Transition table} \\ \hline
  from         & message            & parameters             & to       \\ \hline\hline
  \Idle        & \Request           & $request$              & \Busy     \\ \hline
  \Busy        & \Response          & $response$             & \Done     \\ \hline
\end{tabular}

\section{Single Phase Chain Synchronisation Protocol}
\label{chain-sync-protocol}
\hsref{ouroboros-network/src/Ouroboros/Network/Protocol/ChainSync/Type.hs}
\newcommand{\CanAwait}{\state{CanAwait}}
\newcommand{\MustReply}{\state{MustReply}}
\newcommand{\Intersect}{\state{Intersect}}
\newcommand{\RequestNext}{\msg{RequestNext}}
\newcommand{\AwaitReply}{\msg{AwaitReply}}
\newcommand{\RollForward}{\msg{RollForward}}
\newcommand{\RollBackward}{\msg{RollBackward}}
\newcommand{\FindIntersect}{\msg{FindIntersect}}
\newcommand{\IntersectImproved}{\msg{IntersectImproved}}
\newcommand{\IntersectUnchanged}{\msg{IntersectUnchanged}}

\subsection{Description}
The chain synchronisation protocol is used by the block chain consumer
to replicate the producer's block chain locally. As it is polymorphic in blocks,
it supports a family of Ouroboros protocols.
The chain synchronisation protocol is used by clients which synchronise just headers (node-to-node),
and also clients which synchronize actual block chain (e.g., node-to-wallet).
(See Figure~\ref{node-diagram-concurrency}.)

A node communicates with several upstream and downstream nodes.
A node runs an independent client instance and a independent server instance for every
other node it communicates with.

\subsection{State Machine}

\begin{tabular}{|l|l|}
  \hline
  \multicolumn{2}{|c|}{Agency} \\ \hline
  Client has Agency & \Idle \\  \hline
  Server has Agency & \CanAwait, \MustReply, \Intersect \\ \hline
\end{tabular}

\begin{figure}[H]
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4.5cm, semithick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]
  \node[state, green, initial]                            (Idle)      {\Idle};
  \node[state, right of=Idle]                             (Done)      {\Done};
  \node[state, blue, below left of=Idle]                  (CanAwait)  {\CanAwait};
  \node[state, blue, right of=CanAwait]                   (MustReply) {\MustReply};
  \node[state, blue, above of=Idle]                       (Intersect) {\Intersect};

  \draw (Idle)         edge[left, bend right]      node{\RequestNext}           (CanAwait);
  \draw (CanAwait)     edge[below, bend right]     node{\AwaitReply}            (MustReply);
  \draw (CanAwait)     edge[above,bend right=45]     node{\RollForward}           (Idle);
  \draw (MustReply)    edge[right,bend right=45]     node{\RollForward}           (Idle);
  \draw (CanAwait)     edge[above,bend right=80]     node{\RollBackward}          (Idle);
  \draw (MustReply)    edge[right,bend right=80]     node{\RollBackward}          (Idle);
  \draw (Idle)         edge[right, bend right]    node{\FindIntersect}         (Intersect);
  \draw (Intersect)    edge[above, bend right=45]    node[below = 4mm]{\IntersectImproved}     (Idle);
  \draw (Intersect)    edge[above, bend right=80]    node[above = 4mm]{\IntersectUnchanged}    (Idle);
  \draw (Idle)         edge[above]                node{\MsgDone}                  (Done);
\end{tikzpicture}
\caption{State machine of the chain sync protocol.}
\label{chain-sync-automata}
\end{figure}

The protocol uses the following messages:
\begin{description}
\item [\RequestNext]
      Request the next update from the producer.
      The response can be a roll forward, a roll back or wait.
\item [\AwaitReply]
      Acknowledge the request but require the consumer to wait for the next update.
      This means that the consumer is synced with the producer, and
      the producer is waiting for its own chain state to change.
\item [\RollForward~{\boldmath $(header,point)$}]
      Tell the consumer to extend their chain with the given $header$.
      The message also tells the consumer about the $head$ point of the producer.
\item [\RollBackward~{\boldmath $(point_{old},point_{head})$}]
      Tell the consumer to roll back to a given $point_{old}$ on their chain.
      The message also tells the consumer about the current head $point_{head}$ of the producer.
\item [\FindIntersect~{\boldmath $\langle point_{head} \rangle $}]
      Ask the producer to try to find an improved intersection point between
      the consumer and producer's chains.
      The consumer sends a sequence {\boldmath $\langle point \rangle $}
      and it is up to the producer
      to find the first intersection point on its chain and send it back to the consumer.
\item [\IntersectImproved~{\boldmath $(point_{intersect},point_{head})$}]
      The reply to the consumer about an intersection found, but {\bf only} if this
      is an improvement over previously established intersection point.
      The consumer can decide weather to send more points.
      The message also tells the consumer about the head point of the producer.
\item [\IntersectUnchanged~{\boldmath $(point_{head})$}]
      The reply to the consumer that no intersection was found: none of the
      points the consumer supplied are on the producer chain.
      The message also tells the consumer about the head point of the producer.
\item [\MsgDone]
      Terminate the protocol.
\end{description}

\begin{tabular}{|l|l|l|l|}
  \hline
  \multicolumn{4}{|c|}{Transition table} \\ \hline
  from state & message             & parameters                          & to state    \\ \hline\hline
  \Idle      & \RequestNext        &                                     & \CanAwait   \\ \hline
  \Idle      & \FindIntersect      & $\langle point\rangle$              & \Intersect  \\ \hline
  \Idle      & \MsgDone            &                                     & \Done       \\ \hline
  \CanAwait  & \AwaitReply         &                                     & \MustReply  \\ \hline
  \CanAwait  & \RollForward        & $header$, $point_{head}$            & \Idle       \\ \hline
  \CanAwait  & \RollBackward       & $header$, $point_{head}$            & \Idle       \\ \hline
  \MustReply & \RollForward        & $header$, $point_{head}$            & \Idle       \\ \hline
  \MustReply & \RollBackward       & $point_{old}$, $point_{head}$       & \Idle       \\ \hline
  \Intersect & \IntersectImproved  & $point_{intersect}$, $point_{head}$ & \Idle       \\ \hline
  \Intersect & \IntersectUnchanged & $point_{head}$                      & \Idle       \\ \hline

\end{tabular}

\subsection{Implementation of the Chain Producer}
The trade-offs between the robustness and efficiency of possible chain-sync protocols are
discussed in Section~\ref{chain-sync-discussion}.
This section describes a state-full implementation of a chain producer that is suitable for a setting where
the producer cannot trust the chain consumer.
An important requirement, in this setting,
is that a chain consumer must never be able to cause excessive resource use on the producer side.
The presented implementation meets this requirement.
It strictly only uses a constant amount of memory to store the state, that the producer maintains
per chain consumer.  This protocol is only used to reproduce the producer
chain locally by consumer.  By running many instances of this protocol against
different peers, a node can reproduce chains in the network and
do chain selection which by design is not part of this protocol.  An important
note is that the consumer's chain described in this section is the chain
reproduced by the client of chain-sync protocol, rather than the chain of that
node.

We call the state which the producer maintains about the consumer, the $read~pointer$.
The $read~pointer$ basically tracks what the producer knows about the head of
the consumer's chain without storing it locally.
It points to a block on the current chain of the chain producer.
The $read~pointers$ are part of the shared state of the node (Figure~\ref{node-diagram-concurrency}) and
$read~pointer$s are concurrently updated by the thread that runs the chain-sync mini-protocol and the
chain tracking logic of the node itself.

We first describe how the mini-protocol updates a $read~pointer$ and later address what happens in case
of a fork.
The chain producer assumes that a consumer, which has just connected,
only knows the genesis block and initializes the $read~pointer$ of that consumer
with a pointer to the genesis block on its chain.

\subparagraph{Downloading a chain of blocks}
A typical situation is when the consumer follows the chain of the producer but is not yet at the head of the
chain (this also comprises a consumer booting from genesis).
In this case, the protocol follows a simple, consumer-driven, request-response pattern.
The consumer sends \RequestNext~messages to ask for the next block.
If the $read~pointer$ is not yet at the head of the chain,
the producer replies with a \RollForward{} and advances the $read~pointer$ to
the next block (optimistically assuming that the client will update its chain
accordingly).
The \RollForward~message contains the next block and also the head-point of the producer.
The protocol follows this pattern until the $read~pointer$ reaches the end of its chain.

\begin{figure}[h]
\pgfdeclareimage[height=7cm]{read-pointer-consumer-driver}{read-pointer-consumer-driven.pdf}
\begin{center}
\pgfuseimage{read-pointer-consumer-driver}
\end{center}
\caption{Consumer driven block download.}
\label{read-pointer-consumer-driver}
\end{figure}

\subparagraph{Producer driven updates}
If the $read~pointer$ points to the end of the chain and the producer receives
a \RequestNext{}
the consumers chain is already up to date.
The producer informs the consumer with an \AwaitReply{} that no new data is available.
After receiving a \AwaitReply, the consumer just waits for a new message and the producer keeps agency.
The \AwaitReply~switches from a consumer driven phase to a producer driven phase.

The producer blocks until new data becomes available.
When a new block is available, the producer will
reply with a \RollForward{} message and give agency back to the consumer.
The producer can also get unblocked when its node switches to a new chain fork.

\subparagraph{Producer switches to a new fork}
The node of the chain producer can switch to a new fork at any time, independent of the
state machine.
A chain switch can cause an update of the $read~pointer$,
which is part of the mutable state that is shared between the thread that runs
the chain sync protocol and the thread that implements the chain following logic of the node.
There are two cases:

1) If the $read~pointer$ points to a block that is on the common prefix of the new
fork and the old fork, no update of the $read~pointer$ is needed.

2) If the $read~pointer$ points to a block that is no longer part of the chain,
that is followed by the node,
the $read~pointer$ is set to the last block that is common between the new and the old chain.
The node also sets a flag that signals the chain-sync thread to reply with a \RollBackward~instead
of a \RollForward.
Finally the producer thread must unblock if it is in the \MustReply~state.

\begin{figure}[h]
\pgfdeclareimage[height=5cm]{read-pointer-rollback}{read-pointer-rollback.pdf}
\begin{center}
\pgfuseimage{read-pointer-rollback}
\end{center}
\caption{$read~pointer$ update for a fork switch in case of a rollback.}
\label{read-pointer-rollback}
\end{figure}

Figure~\ref{read-pointer-rollback} illustrates a fork switch that requires an update of the $read~pointer$
for one of the chain consumers, i.e. an example for case 2).
Before the switch, the $read~pointer$ of the consumer points to block $0x660f$.
The producer switches to a new chain with the head of the chain at block $0xcdf0$.
The node must update the $read~pointer$ to block $0xfa40$ and the next message to the consumer will be a \RollBackward.

Note, that a node typically communicates with several consumers. For each consumer it runs an independent
version of the chain-sync-protocol state machine in an independent thread and with its own $read~pointer$.
Each of those $read~pointer$s has to be updated independently and for each consumer
either case 1) or case 2) can apply.

\subparagraph{Consumer starts with an arbitrary fork}
Typically, the consumer already knows some fork of the block chain when it
starts to track the producer.
The protocol provides an efficient method to search for the longest common prefix (called intersection here)
between the fork of the producer and the fork that is known to the consumer.

To do so, the consumer sends \FindIntersect~message with a list of chain
points which belong to its node chain.
If the producer does not know any of the points it replies with \IntersectUnchanged.
Otherwise it replies with \IntersectImproved~and the best (i.e. the newest) of the points that it knows
and also updates the $read~pointer$ accordingly.
For efficiency, the consumer should use a binary search scheme to search for the longest common
prefix.

It is advised that the consumer always starts with \FindIntersect~in a fresh connection
and it is free to use \FindIntersect~at any time later as seems beneficial.
If the consumer does not know anything about the consumers chain,
it can start the search with the following list of points:
$\langle point(b), point(b-1), point(b-2), point(b-4), point (p-8),\ldots \rangle$
where $point(b-i)$ is the point of the $i$th predecessor of block $b$ and
$b$ is the head of the consumer fork.
\wip{(Note, that the maximum depth of a fork in Ouroboros is bounded).}

\subsection{Implementation of the Chain Consumer}
In principle, the chain consumer has to guard against a malicious chain producer
as much as the other way around.
However, two aspects of the protocol play in favour of the consumer here.
\begin{itemize}
  \item The protocol is basically consumer driver, i.e. the producer has simply no way to send unsolicited
data to the consumer (within the protocol).
  \item The consumer can verify the response data itself.
\end{itemize}
Here are some cases to consider:
\begin{description}
\item[\FindIntersect~Phase] The consumer and the producer play a number guessing game, the consumer can easily detect inconsistent behaviour.
\item[The producer replies with a \RollForward] The consumer can verify the block itself
  (with the help for the ledger layer)
  (The consumer may need to download the block first, if the protocol only sends block headers.)
\item[The producer replies with a \RollBackward] The consumer tracks several producers.
  If the producer sends false \RollBackward~messages the consumer's node
  will, at some point, just switch to a longer chain fork.
\item[The Producer is just passive/slow] The consumer's node will switch to
  a longer chain coming from another producer via another instance of
    chain-sync protocol.
\end{description}
\wip{This should be explained in detail}

\section{Block Fetch Protocol}
\label{block-fetching-protocol}

\hsref{ouroboros-network/src/Ouroboros/Network/Protocol/BlockFetch/Type.hs}
\renewcommand{\Idle}{\state{Idle}}
\renewcommand{\Busy}{\state{Busy}}
\newcommand{\Streaming}{\state{Streaming}}
\renewcommand{\Done}{\state{Done}}
\newcommand{\RequestRange}{\msg{RequestRange}}
\newcommand{\StartBatch}{\msg{StartBatch}}
\newcommand{\NoBlocks}{\msg{NoBlocks}}
\newcommand{\Block}{\msg{Block}}
\newcommand{\BatchDone}{\msg{BatchDone}}
\newcommand{\ClientDone}{\msg{ClientDone}}

\subsection{Description}

The block fetching mechanism enables a node to download a range of blocks.

\subsection{State machine}

\begin{tabular}{|l|l|}
  \hline
  \multicolumn{2}{|c|}{Agency} \\ \hline
  Client has Agency & \Idle            \\  \hline
  Server has Agency & \Busy, \Streaming \\ \hline
\end{tabular}

\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4.5cm, semithick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]
  \node[state, green, initial]                            (Idle)      {\Idle};
  \node[state, right of=Idle]                             (Done)      {\Done};
  \node[state, blue, below left of=Idle]                  (Busy)      {\Busy};
  \node[state, blue, right of=CanAwait]                   (Streaming) {\Streaming};

  \draw (Idle)         edge[above]                node{\ClientDone}                  (Done);
  \draw (Idle)         edge[left,bend right]      node{\RequestRange}                (Busy);
  \draw (Busy)         edge[above,bend right]     node{\NoBlocks}                    (Idle);
  \draw (Busy)         edge[below]                node{\StartBatch}                  (Streaming);
  \draw (Streaming)    edge[loop right]           node{\Block}                       (Streaming);
  \draw (Streaming)    edge[right]                node{\BatchDone}                   (Idle);
\end{tikzpicture}

\begin{description}
\item [\RequestRange~{\boldmath $range$}]
  The client requests a {\boldmath $range$} of blocks from the server.
\item [\NoBlocks]
  The server tells the client that it does not have blocks.
\item [\StartBatch]
  The server starts block streaming.
\item [\Block~{\boldmath $body$}]
  Stream a single block's body.
\item [\BatchDone]
  The server ends block streaming.
\item [ClientDone]
  The client terminates the protocol.
\end{description}

Transition table:

\begin{tabular}{|l|l|l|l|}
  \hline
  \multicolumn{4}{|c|}{Transition table} \\ \hline
  from state   & message             & parameters             & to state    \\ \hline\hline
  \Idle        & \ClientDone         &                        & \Done       \\ \hline
  \Idle        & \RequestRange       & $range$                & \Busy       \\ \hline
  \Busy        & \NoBlocks           &                        & \Idle       \\ \hline
  \Busy        & \StartBatch         &                        & \Streaming  \\ \hline
  \Streaming   & \Block              & $body$                 & \Streaming  \\ \hline
  \Streaming   & \BatchDone          &                        & \Idle       \\ \hline
\end{tabular}

\section{Transaction Submission Mini Protocol}
\hsref{src/Ouroboros/Network/Protocol/TxSubmission/Type.hs}
\label{tx-submission-protocol}
\newcommand{\SendTx}{\state{SendTx}}
\newcommand{\SendHashes}{\state{SendHashes}}
\newcommand{\GetTx}{\state{GetTx}}
\newcommand{\GetHashes}{\state{GetHashes}}
\newcommand{\Tx}{\state{Tx}}

\subsection{Description}
A core node uses the transaction submission protocol to retrieve transactions from
a wallet.
The node runs the client instance of the transaction submission protocol and the wallet runs
the server instance.
Note, that the protocol is {\bf not} used to forward transactions from one core node to an other.

\subsection{State machine}

\begin{tabular}{|l|l|}
  \hline
  \multicolumn{2}{|c|}{Agency} \\ \hline
  Client has Agency & \Idle \\  \hline
  Server has Agency & \SendHashes,~\SendTx \\ \hline
\end{tabular}

\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm, semithick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]
  \node[state, green, initial]                                   (Idle)       {\Idle};
  \node[state, right of=Idle]                                    (Done)       {\Done};
  \node[state, blue, above of=Idle]                              (SendTx)     {\SendTx};
  \node[state, blue, below of=Idle]                              (SendHashes) {\SendHashes};

  \draw (Idle)         edge[]      node{\MsgDone}           (Done);
  \draw (Idle)         edge[right, bend right=45]      node{\GetTx}               (SendTx);
  \draw (Idle)         edge[left, bend right=45]       node{\GetHashes}           (SendHashes);
  \draw (SendTx)       edge[left, bend right=45]      node{\Tx}            (Idle);
  \draw (SendHashes)   edge[right, bend right=45]       node{\SendHashes}    (Idle);

\end{tikzpicture}

Messages of the protocol:
\begin{description}
\item [\GetHashes~{\boldmath $n$}]
      The client asks for transaction hashes from the server.
      $n$ is the maximum number of hashes it will accept.
\item [\SendHashes~{\boldmath $hashes$}]
      The server replies with a list of $hashes$.
\item [\GetTx~{\boldmath $hash$}]
      The client request the transaction with $hash$.
      The client must only send \GetTx~messages with hashes it has received
      in a \SendHashes~message from the this server.
\item [\Tx~{\boldmath $tx$}]
      The server sends transaction $tx$ to the client.
\item [\MsgDone]
      The client terminates the mini protocol.
\end{description}

\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{Transition table} \\ \hline
  from        & message/event      & parameters            & to          \\ \hline\hline
  \Idle       & \GetHashes         & $n$                   & \SendHashes  \\ \hline
  \SendHashes & \SendHashes        & $hashes$              & \Idle \\ \hline
  \Idle       & \GetTx             & $hash$                & \SendTx \\ \hline
  \SendTx     & \Tx                & $tx$                  & \Idle \\ \hline
  \Idle       & \MsgDone           &                       & \Done\\ \hline
\end{tabular}

\subsection{Client and Server Implementation}
The transaction submission protocol is strictly only used
by a (core) node to retrieve transactions from a wallet (or a similar source of transactions).
It is {\bf not} used to relay transactions from node to node.
It is important to note, that core nodes only ever run client instances of the protocol,
while the server instance is implemented by the wallet.
Therefore only the client instance of the protocol has to be save a against an adversarial server
and not the other way around.

A client can use the \GetHashes~$n$ message to request a limited number transaction hashes from the server
and, after that, downloads the transactions one after the other.
When a new transaction arrives (within a \Tx~message), the client uses the consensus layer to
validate the transaction.
If the transaction is valid, the node puts it in the mempool and otherwise it is discarded.
Depending on the rate of incoming transactions and on the frequency of invalid transactions,
a client can implement a suitable policy to throttle down a server or to disconnect altogether.

\section{Pipelining of Mini Protocols}
Protocol pipelining is a technique that allows to improve the performance of some protocols.
The underlying idea is that a client, which wants to perform several requests,
just transmits those request in sequence without blocking and waiting for the reply from the server.
In the reference implementation, pipelining is used by the clients of all mini protocol except Chain-Sync.
Those mini protocols follow a request response pattern that is amenable to pipelining such
that pipelining becomes a feature of the client implementation that does not require any
modifications of the server implementation.

As an example, lets consider the Block-Fetch mini protocol.
When a client follows the protocol and sends a sequence of \RequestRange~messages to the server
the data stream from the client to the server will only consist of \RequestRange~messages
(and a final \ClientDone~message) and no other message types.
The server can simply follow the state machine of the protocol and process the messages in turn,
regardless whether the client uses pipelining or not.
The MUX/DEMUX layer (Section~\ref{multiplexing-section}) guaranties that messages of the same mini protocol
are delivered in transmission order, and therefore the client can determine
which response belongs to which request.

The MUX/DEMUX layer also provides a fixed size buffer between the egress of DEMUX and the ingress
of mini protocol thread.
The size of this buffer is a protocol parameter that determines how many messages
a client can send before waiting for a reply from the server.
The protocol requires that a client must never cause a overrun of theses buffers on a server node.
If a message arrives at the server that would cause the buffer to overrun,
the server treats this case as a protocol violation of the peer (and closes the connection to the peer).
The buffer sizes are listed in Table~\ref{bla} in Section~\ref{blub}.

\section{DeltaQ Mini Protocol}
\wip{
  WIP : Explain DeltaQ measurement back pressure and how we deal with slow connection.
  See Section~\ref{deltaq-discussion}.
The DeltaQ mini protocol does not transmit is own messages.
Instead it relies on the time stamps that the multiplexing layer (Section~\ref{multiplexing}) adds
to the messages of other mini protocols.
}

\chapter{Connection Management}
\label{connection-management}
\todo[inline]{Add a section which describes version negotiation protocol.}

\section{The Multiplexing Layer}
\label{multiplexing-section}
Multiplexing is used to run several mini protocols in parallel over a single
channel (for example a single TCP connection).
Figure~\ref{mux-diagram} shows an example of two nodes, each running three
mini protocols and a multiplexer/de-multiplexer.
All the data, that is transmitted between the nodes, passes through the MUX/DEMUX of the nodes.
There is a fixed pairing of the mini protocol instances, i.e. each mini protocol instance only
communicates with its dual instance.

\begin{figure}[h]
\pgfdeclareimage[height=5cm]{mux-diagram}{mux-diagram.pdf}
\begin{center}
\pgfuseimage{mux-diagram}
\end{center}
\caption{Data flow though the multiplexer and de-multiplexer}
\label{mux-diagram}
\end{figure}

The implementation of the mini protocol also handles the serialisation and de-serialisation of its messages.
The mini protocols write chunks of bytes to the MUX and read chunks of bytes from the DEMUX.
The MUX reads the data from the mini protocols, splits the data into segments, adds a segment header
and transmits the segments to the DEMUX of its peer.
The DEMUX uses the segment's headers to reassemble the byte streams for the mini protocols on its side.
The multiplexing protocol itself is completely agnostic to the structure of the multiplexed data.

\subsection{Wire Format}
\hsref{ouroboros-network/src/Ouroboros/Network/Mux/Egress.hs}
\begin{table}[h]
\centering
\begingroup
\setlength{\tabcolsep}{3pt}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
  0&1&2&3&4&5&6&7&8&9&0&1&2&3&4&5&6&7&8&9&0&1&2&3&4&5&6&7&8&9&0&1 \\ \hline
  \multicolumn{32}{|c|}{Transmission Time} \\ \hline
  \multicolumn{1}{|c|}{$M$}
  &\multicolumn{15}{|c|}{Mini Protocol ID}
  &\multicolumn{16}{|c|}{Payload-length $n$} \\ \hline
  \multicolumn{32}{|c|}{} \\
  \multicolumn{32}{|c|}{Payload of $n$ Bytes} \\
  \multicolumn{32}{|c|}{} \\ \hline
\end{tabular}
\endgroup
\caption{Multiplexing-segments}
\label{segment-header}
\end{table}

Table~\ref{segment-header} shows the layout of the data segments of the multiplexing protocol
(big-endian bit order).
The segment header contains the following data:
\begin{description}
\item[Transmission Time]
  The transmission time is a time stamp based the wall clock of the peer with a
  resolution of one microsecond.
\item[Mini Protocol ID] The unique ID of the mini protocol as in Table~\ref{mini-protocol-id}.
\item[Payload Length] The payload length is the size of the segment payload in Bytes.
  The maximum payload length that is supported by the multiplexing wire format is $2^{16}-1$.
  Note, that an instance of the protocol can choose a smaller limit for the size of segments it transmits.
\item[Mode] The single bit $M$ (the mode) is used to distinct the dual instances of a mini protocol.
  The mode is set to $0$ in segments from the initiator, i.e. the side that initially has agency and
  $1$ in segments from the responder.
\end{description}

\subsection{Fairness and Flow-Control in the Multiplexer}
The Shelley network protocol requires that the multiplexer uses a fair scheduling of the mini protocols.

The reference Haskell implementation of multiplexer uses a round-robin-schedule of the mini protocols
to choose the next data segment to transmit.
If a mini protocol does not have new data available when it is scheduled, it is skipped.
A mini protocol can transmit at most one segment of data every time it is scheduled
and it will only be rescheduled immediately if no other mini protocol has data available.
Each mini protocol is implemented as a separate Haskell thread.
These threads can signal the multiplexer at any time that they have new data available.

From the point of view of the mini protocols, there is a one-message buffer between the egress of
the mini protocol and the ingress of the multiplexer.
The mini protocol will block when it sends a message and the buffer is full.

A concrete implementation of a multiplexer may use a variety of data structures and heuristics to
yield the overall best efficiency.
For example, although, the multiplexing protocol itself is agnostic to the underlying structure of
the data, the multiplexer may try to avoid to split small mini protocol messages into two segments.

\subsection{Flow-control and Buffering in the Demultiplexer}
The demultiplexer eagerly reads data from its ingress.
There is a fixed size buffer between the egress of the demultiplexer and the ingress of
the mini protocols.
Each mini protocol implements its own mechanism for flow control which guaranties that this buffer
never overflows.
If the demultiplexer detects an overflow of the buffer, it means that the peer violated the
protocol and the MUX/DEMUX layer shuts down the connection to the peer.
The size of the buffers is listed in Table~\ref{demux-buffers}.

\section{Setup, Shutdown and Management of Connections}
\label{peer-setup-section}
In addition to the exchange of blocks and transactions, as required by Ouroboros,
the network layer also handles several administrative tasks.
This section describes the parts of the protocol that deal with setting up, shutting down and
managing a connection between two peers.
In this section, we use the term {\bf connection} or {\bf bearer} for the multiplexing-layer object
that manages the mini protocol threads, the buffers and the OS-level connection
(for example TCP socket) that deal with one peer of the node.

The multiplexing layer (Section~\ref{multiplexing-section}) is the central crossing between
the mini protocols and the network channel.
Therefore, the reference implementation takes the approach
to implement the functions for connection management in the same part of the source code
that also implements the multiplexing layer itself.

This section describes the protocol and sketches a possible implementation.
Roughly the implementation performs the following tasks:
\begin{itemize}
\item Open a socket/ acquire resources from the OS.
\item Verify protocol Versions.
\item Spawn the threads that run the mini protocols.
\item Measure transmission times and amount of in-flight data.
\item Catch exceptions that are thrown by the mini protocols.
\item Shutdown the connection in case of an error.
\item Handle a shutdown request from the peer.
\item Shutdown the threads that run the mini protocols.
\item Close Socket/ free resources.
\end{itemize}

\newcommand{\Larval}{\state{Larval}}
\newcommand{\Connected}{\state{Connected}}
\newcommand{\Mature}{\state{Mature}}
\newcommand{\Dying}{\state{Dying}}
\newcommand{\Dead}{\state{Dead}}

\section{Life Cycle of a Connection}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm, semithick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]
  \node[state, initial]                           (Larval)       {\Larval};
  \node[state, below of=Larval]                   (Connected)    {\Connected};
  \node[state, below of=Connected]                (Mature)       {\Mature};
  \node[state, below of=Mature]                   (Dying)        {\Dying};
  \node[state, below of=Dying]                    (Dead)         {\Dead};
  \draw (Larval)            edge[]      node{OS connect}         (Connected);
  \draw (Connected)         edge[]      node{agree protocol versions, start mini protocols}           (Mature);
  \draw (Mature)         edge[]      node{termination of any mini protocol}           (Dying);
  \draw (Dying)         edge[]      node{transmission of messages that are already buffered,OS disconnect}           (Dead);
\end{tikzpicture}



A connection passes through several stages during its life cycle.
\begin{description}
\item[\Larval]    The connection exists but nothing has been initialised yet.
\item[\Connected] The OS-level primitives (sockets or pipes) are connected.
\item[\Mature] The mini protocols are running.
\item[\Dying]  One of the mini protocols has terminated.
\item[\Dead] The connection has been terminated.
\end{description}

\newcommand{\InitReq}{\msg{InitReq}}
\newcommand{\InitRsp}{\msg{InitRsp}}
\newcommand{\InitFail}{\msg{InitFail}}

\subsection{Setup of a Connection with the Mux-Control Protocol}
The Mux-Control protocol is used to negotiate the versions of the mini
protocols that are used by the client and the server.
It is run exactly once after the connection enters the \Connected-state and before
any of the mini protocols is initialised.
In a run of the Mux-Control protocol the peers exchange only two messages:
The client request to connect with a \InitReq~message that contains a map of all supported protocol versions
and the server replies either with a \InitRsp~message containing the negotiated version
or a \InitFail~message.

Section~\label{included-cddl} contains the CDDL-specification of the binary format of the messages.
Each message is transmitted and within single MUX segment, i.e. with a proper segment header,
but, as the MUX/DEMUX is not yet running the messages must not be split into multiple segments.

\noindent\hsref{ouroboros-network/src/Ouroboros/Network/Mux/Types.hs}
\newline\hsref{ouroboros-network/src/Ouroboros/Network/NodeToNode.hs}
\newline\hsref{ouroboros-network/src/Ouroboros/Network/NodeToClient.hs}

\todo{Update needed !}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|}
  \hline
  ID & Mini Protocol                         & NtN  & NtC \\ \hline
  0  & MUX-Control                           & Yes  & Yes \\ \hline
  1  & DeltaQ                                & Yes  & Yes \\ \hline
  2  & Chain-Sync instantiated to headers    & Yes  & No \\ \hline
  3  & Block-Fetch                           & Yes  & No  \\ \hline
  4  & Transaction-Submission                & No   & Yes  \\ \hline
  5  & Chain-Sync instantiated to blocks     & No   & Yes  \\ \hline
\end{tabular}
\caption{Mini Protocol IDs}
\label{mini-protocol-id}
\end{table}

\subsection{Default Mini Protocol Sets for Node-to-Node and Node-to-Client}
Table~\ref{mini-protocol-id} show which mini protocols are enabled for node-to-node
and node-to-client communication.
Mux-Control and DeltaQ are enabled for all connections.
The communication between two full nodes (NtN) is fully symmetric.
Both nodes run initiator and responder instances of the
Chain-Sync, the Block-Fetch and the Transaction-Submission protocol.
Node-to-Client (NtC) is a connection between a full node and a client that does not take part in
Ouroboros protocol itself and only consumes data, for example a wallet or a block chain explorer.
In a NtC setup, the node only runs the producer side of the Chain-Sync protocol and the client only the
consumer side.
The Chain-Sync protocol is polymorphic in the type of blocks that are transmitted.
NtN uses a Chain-Sync instance which only transmits block headers, while the NtC instance transmits
full blocks.
The two variants of Chain-Sync use different protocol IDs.

% Ping Pong ?
%

\subsection{Time Measurement}

\subsection{Error Handling}
When a mini protocol thread detects that a peer violates the mini protocol it throws an exception.
The MUX-layer catches the exceptions from the mini protocol threads and shuts down the connection.


\chapter{Peer Discovery and Peer Selection}
\wip{
  Peer discovery and peer selection are relatively independent for the core components of a node.
  In a first iteration, it may be enough to specify what data is going from
  the peer selection algorithm to
  the core components and what data is going from the core components to the peer selection.
  Also peer discovery and peer selection are relatively independent from each other.
  }

\chapter{Infrastructure}
\label{infrastructure}
\wip{
  WIP: Specific assumptions about the infrastructure that are relevant for the discussion.
}

\section{Internet}
\section{Network Topology}
\section{DNS,NTP}
\section{Topographical distribution of block creating nodes}
\section{TCP}
\section{IP}
MTU
\section{Operating Systems}
\section{Firewall}
\section{Nodes and Hosting}

\chapter{Haskell}
While the network protocol itself can be implemented in many programming languages,
it has been developed in parallel with a Haskell reference implementation.
In addition to the language agnostic protocol description in the other parts of this document,
this section discusses key aspects of the Haskell implementation.
This section is most useful for people who work with the Haskell reference implementation and
may give some extra insights for anybody who is interested in implementing the
network component.
For understanding the protocol, it is save to skip this section.
\section{Constant Memory Consumption}
\section{The State Machine Framework}
\label{Haskell-state-machine}

\chapter{Discussion}
\section{Related Work}
\subsection{Other Crypto Currencies}
\subsubsection{PoW Systems}
\subsubsection{PoS Systems}
\subsection{Generic Peer to Peer Systems}
\subsection{Formal Correctness}
\wip{
  The correctness of distributed and concurrent systems has been studied intensively for decades.
\begin{description}
\item [Safety properties]
  Prove that a bad thing will never happen.
  \begin{itemize}
  \item Coins cannot be stolen
  \item Preservation of Money
  \item Nodes will not run out of Memory
  \item (Property: Current state is valid) will always hold / never fail
  \end{itemize}
\item[Liveness properties]
  Prove that a desired event will happen.
  \begin{itemize}
  \item Message will be delivered
  \item Consensus will be reached
  \item Transaction will be confirmed
  \item Fairness : the desired event will happen in time. One does not have to wait forever
  \item Starvation
  \item Deadlocks
  \end{itemize}
\item[Temporal logic]
  Tailor made logic for analysing concurrent systems.
  \begin{itemize}
  \item Argue about the temporal order of events in transition systems.
  \item Express safety properties.
  \item Express liveness properties.
  \item Express Fairness.
  \item Prove with model checkers.
  \item Refinement properties.
  \item CTL computation tree logic (safety)
  \item LTL linear time logic (fairness)
  \end{itemize}
\item[Time]
  How does a concurrent system deal with time ?
  \begin{itemize}
  \item Physical clocks / Wall clock time
  \item Logical clocks / Vector clocks / order of events
  \item Order of events : Before , Concurrent, After
  \item Hybrid approaches, Ouroboros, slot-times
  \end{itemize}
\item[Session Types]
     Model protocols and transition systems in a type system.
\item[Pi-calculus]
\item[Process algebras]
\end{description}
}

\wip{WIP: Poldercast,etc}
\section{Overview}
\section{Design Discussion}
\subsubsection{Why distinguish between node to node and node-to-consumer IPC}
\label{why_distinguish_protocols}
We use two different sets of protocols for these two use cases.

\begin{description}
\item[node-to-node] IPC between nodes that are engaged in the high level Ouroboros
      blockchain consensus protocol.
\item[node-to-consumer] IPC between a Cardano node and a `chain consumer' component such as a
      wallet, explorer or other custom application.
\end{description}

This section describes the differences between those two variants of IPC and why both use
different protocols.

The node-to-node protocol is conducted in a P2P environment
with very limited trust between peers. The node-to-node protocol utilises
store-and-forward over selected \emph{bearers} which form the underlying
connectivity graph. A concern in this setting are asymmetric resource
consumption attacks. Ease of implementation is a nice to have, but is
subordinate to the other hard constraints.

A node-to-consumer protocol is intended to support blockchain applications
like wallets and explorers, or Cardano-specific caches or proxies. The setting
here is that a consumer trusts a node (a `chain producer') and just wants to
catch up and keep up with the blockchain of that producer. It is assumed that
a consumer only consumes from one producer (or one of a related set of
producers), so unlike in the node-to-node protocol there is no need to choose
between different available chains. The producer may still not fully trust the
consumer and does not want to be subject to highly asymmetric resource
consumption attacks. In this use case, because of the wider range of
applications that wish to consume the blockchain, having some options that are
easy to implement is more important, even if this involves a trade-off with
performance. That said, there are also use cases where tight integration is
possible and making the most efficient use of resources is more desirable.

There are a number of applications that simply want to consume the blockchain,
but are able to rely on an upstream trusted or semi-trusted Cardano consensus
node. These applications do not need to engage in the full consensus protocol,
and may be happy to delegate the necessary chain validation.

Examples include 3rd party applications that want to observe the blockchain,
examples being business processes triggered by transactions or analytics.  It
may also include certain kinds of light client that wish to follow the
blockchain but not do full validation.

Once one considers a node-to-consumer protocol as a first class citizen then it
opens up opportunities for different system architecture choices.
The architecture of the original Cardano Mainnet release was entirely homogeneous:
every node behaved the same, each trusted nothing but itself and paid the full
networking and processing cost of engaging in the consensus protocol.  In
particular everything was integrated into a single process: the consensus
algorithm itself, serving data to other peers and components such as the wallet
or explorer. If we were to have a robust and efficient node-to-consumer protocol
then we can make many other choices.

With an efficient \emph{local} IPC protocol we can have applications
like wallets and explorers as separate processes. Even for tightly
integrated components it can make sense to run them in separate OS
processes and the associated OS management tools. Not only is the
timing constraints for a consensus node are much easier to manage when
it does not have to share CPU resources with chain consumers, but it
enables the use of operating system features to give finer control
over resource consumption for sophisticated end-users.  There have
been cases in production where a highly loaded wallet component takes
more than its allowed allocation of CPU resources and causes the local
node to miss its deadlines.  By giving a consensus node a dedicated
CPU core it becomes more plausible to provide the necessary hard real
time guarantees. In addition, scaling on multi-core machines is
significantly easier with multiple OS processes than with a
multi-threaded OS process with a shared-heap. This could allow for
larger capacity Cardano relay deployments where there are multiple
network facing proxy processes that all get their chain from a single
local consensus node.

With an efficient \emph{network} IPC protocol we can do similar things
but extend it across multiple machines. This permits: large
organisations to achieve better alignment with their security
policies; clusters of relays operated by a single organisation to use
the more efficient (less resource costly) node-to-consumer protocol
instead of the node-to-node protocol; Similarly it allows for wallet
or explorer-like applications that need to scale out, and are able to
make use of a trusted node.

\section{Requirements}
\section{Threat Vectors}
\wip{
\begin{description}
\item [Generic Attacks against IP networks]
\item [Attacks against a specific implementation of the protocol]
\item [Attacks against a specific configuration of the system]
\item [Attacks against the network protocol itself]
\item [Attacks against Ouroboros]
\item [Clever combinations of the above]
\end{description}
}
\subsubsection{Asymptotic Resource Consumption}
\section{Results from Simulations}
\section{Pub Sub}
\section{Of the Shelf Protocols}
\section{Congestion Control}
\subsection{DeltaQ and Back-pressure}
\label{deltaq-discussion}
\wip{WIP: discuss DeltaQ and Back-pressure}

\section{Meta Requirements}
\subparagraph{Work in Progress}
This document is evolved in parallel with the work on the protocol design and
the reference implementation.

\subparagraph{The Document should be Comprehensive}
\begin{itemize}
\item Top down approach.
\item Provide the big picture.
\item Usable as a reference point for a broader discussion.
\item Cover every aspect that is related to network connections.
\item Every aspect should at least have a place in the table of contents.
  If there are holes and parts that are not covered the document should say what is missing.
\item Stand alone readable with links to where missing pieces can be found.
\end{itemize}

\subparagraph{Detailed}
\begin{itemize}
\item Sufficient details to allow for new independent implementations that are compatible with
the reference implementation
\item Language agnostic (it is save to skip the Haskell specific parts)
\item Design discussions
\end{itemize}
\subparagraph{Structured}
\begin{itemize}
\item Parts of the document should be in a logical connection
\end{itemize}
\subparagraph{Workflow}

\appendix
\chapter{CDDL Specification of the Protocol Messages}
\label{CBOR-section}
\hsref{ouroboros-network/src/Ouroboros/Network/Protocol/PingPong/Codec.hs}
\label{included-cddl}
This Sections contains the CDDL\cite{cddl} specification
of the binary serialisation format of the network protocol messages.

To keep this Section in close sync with the actual Haskell implementation
the names of the Haskell identifiers have been reused for the corresponding
CBOR types (with the first letter converted to lower case).
Note, that, for readability, the previous Sections used simplified message identifiers,
for example {\tt RequestNext} instead of {\tt msgRequestNext}, etc.
Both identifiers refer to the same message format.

All transmitted messages satisfy the shown CDDL specification.
However, CDDL, by design, also permits variants in the encoding that are not valid in the protocol.
In particular, the notation ${\tt [} ... {\tt ]}$ in CDDL can be used for both fixed-length  
and variable-length CBOR-list, while only either of the two encodings is valid in the protocol.
We add comments in specification to make clear which encoding must be used.

Note that, in the case of the request response mini protocol (Section~ref{request-response-protocol})
there in only ever one possible kind of message in each state.
This means that there is no need to tag messages at all and protocol can directly transmit the plain
request and response data.

\wip{TODO: test that messages.cddl actually works !}
\lstinputlisting{messages.cddl}
\bibliographystyle{apalike}
\bibliography{references}

\chapter{Key Figures of the Protocol and the P2P Network}
This section list some key figures of the network protocol.
There is a variety of figures that quantify some aspects of the protocol, for example:
\begin{itemize}
\item Configuration parameters that are explicitly set in the protocol.
\item Requirements and performance targets.
\item Implicit assumptions about about network bandwidths, etc.
\item Estimates from simulations and game theoretic results.
\end{itemize}
These figures can be a fixed value, a possible interval, a distribution of values,
or just a rough estimate and typically these figures depend on each other.
The figures in this section are not set in stone, but they should help to give a baseline that helps
to understand the protocol design.

Block chain parameters:\\
\begin{tabular}{p{4cm}p{1cm}p{6cm}p{1cm}} \hline
  maximum block size   & = & 2 M Bytes                                    &  \\ \hline
  slot time            & = & 20s                                          &  \\ \hline
  epoch length         & = & 22600 slots $\simeq$ 5 days                  &  \\ \hline
  intrinsic probability of a fork & = &                                   &  \\ \hline
  K parameter (maximal roll back) & = &                                   &  \\ \hline
\end{tabular}\\

Transaction:\\
\begin{tabular}{p{4cm}p{1cm}p{6cm}p{1cm}} \hline
  size of a transaction                & = &  some K Bytes               &  \\ \hline
  through-put transactions per second   & = &  15                        &  \\ \hline
\end{tabular}\\

Network topology:\\
\begin{tabular}{p{4cm}p{1cm}p{6cm}p{1cm}} \hline
  maximum hops                         & $\le$ &  5                         &  \\ \hline
  maximum number of neighbours          & = &  5                         &  \\ \hline
\end{tabular}

\begin{tabular}{p{4cm}p{1cm}p{6cm}p{1cm}} \hline
  bandwidth stake pool                             & = &  > 100Mbit/s             &  \\ \hline
  bandwidth small stake holder                    & = &  ~ 100Mbit/s             &  \\ \hline
  bandwidth none staking chain consumer          & = &  5                       &  \\ \hline
  latency  between stake pool nodes                & <10 ms                       &  \\ \hline
\end{tabular}\\


\chapter{Nomenclature}
\begin{description}
\item[Adversary / Adversarial Action]
  acting in way to subvert the correct (or performant) operation of the distributed protocol.
  Note that non-performance of certain functions at appropriate times can
  fall into this category.
\item[Core DIF]
  The set of end points that belong to the (major)
  stakepools; (the term DIF taken from RINA\ref{RINA} where it denotes
  a (potentially closed) set of potential participants.
\item[egress,ingress]
\item[head of line blocking]
\end{description}
\end{document}
